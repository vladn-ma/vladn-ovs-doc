.TH "/home/vladn/git/ovs/datapath/linux/flow_table.c" 3 "Mon Aug 17 2015" "ovs datapath" \" -*- nroff -*-
.ad l
.nh
.SH NAME
/home/vladn/git/ovs/datapath/linux/flow_table.c \- 
.SH SYNOPSIS
.br
.PP
\fC#include 'flow\&.h'\fP
.br
\fC#include 'datapath\&.h'\fP
.br
\fC#include <linux/uaccess\&.h>\fP
.br
\fC#include <linux/netdevice\&.h>\fP
.br
\fC#include <linux/etherdevice\&.h>\fP
.br
\fC#include <linux/if_ether\&.h>\fP
.br
\fC#include <linux/if_vlan\&.h>\fP
.br
\fC#include <net/llc_pdu\&.h>\fP
.br
\fC#include <linux/kernel\&.h>\fP
.br
\fC#include <linux/jhash\&.h>\fP
.br
\fC#include <linux/jiffies\&.h>\fP
.br
\fC#include <linux/llc\&.h>\fP
.br
\fC#include <linux/module\&.h>\fP
.br
\fC#include <linux/in\&.h>\fP
.br
\fC#include <linux/rcupdate\&.h>\fP
.br
\fC#include <linux/if_arp\&.h>\fP
.br
\fC#include <linux/ip\&.h>\fP
.br
\fC#include <linux/ipv6\&.h>\fP
.br
\fC#include <linux/sctp\&.h>\fP
.br
\fC#include <linux/tcp\&.h>\fP
.br
\fC#include <linux/udp\&.h>\fP
.br
\fC#include <linux/icmp\&.h>\fP
.br
\fC#include <linux/icmpv6\&.h>\fP
.br
\fC#include <linux/rculist\&.h>\fP
.br
\fC#include <net/ip\&.h>\fP
.br
\fC#include <net/ipv6\&.h>\fP
.br
\fC#include <net/ndisc\&.h>\fP
.br
\fC#include 'vlan\&.h'\fP
.br

.SS "Macros"

.in +1c
.ti -1c
.RI "#define \fBTBL_MIN_BUCKETS\fP   1024"
.br
.ti -1c
.RI "#define \fBMASK_ARRAY_SIZE_MIN\fP   16"
.br
.ti -1c
.RI "#define \fBREHASH_INTERVAL\fP   (10 * 60 * HZ)"
.br
.ti -1c
.RI "#define \fBMC_HASH_SHIFT\fP   8"
.br
.ti -1c
.RI "#define \fBMC_HASH_ENTRIES\fP   (1u << MC_HASH_SHIFT)"
.br
.ti -1c
.RI "#define \fBMC_HASH_SEGS\fP   ((sizeof(uint32_t) * 8) / \fBMC_HASH_SHIFT\fP)"
.br
.in -1c
.SS "Functions"

.in +1c
.ti -1c
.RI "static u16 \fBrange_n_bytes\fP (const struct \fBsw_flow_key_range\fP *range)"
.br
.ti -1c
.RI "void \fBovs_flow_mask_key\fP (struct \fBsw_flow_key\fP *\fBdst\fP, const struct \fBsw_flow_key\fP *\fBsrc\fP, const struct \fBsw_flow_mask\fP *mask)"
.br
.ti -1c
.RI "struct \fBsw_flow\fP * \fBovs_flow_alloc\fP (void)"
.br
.ti -1c
.RI "int \fBovs_flow_tbl_count\fP (const struct \fBflow_table\fP *table)"
.br
.ti -1c
.RI "static struct flex_array * \fBalloc_buckets\fP (unsigned int n_buckets)"
.br
.ti -1c
.RI "static void \fBflow_free\fP (struct \fBsw_flow\fP *flow)"
.br
.ti -1c
.RI "static void \fBrcu_free_flow_callback\fP (struct rcu_head *rcu)"
.br
.ti -1c
.RI "static void \fBrcu_free_sw_flow_mask_cb\fP (struct rcu_head *rcu)"
.br
.ti -1c
.RI "void \fBovs_flow_free\fP (struct \fBsw_flow\fP *flow, \fBbool\fP deferred)"
.br
.ti -1c
.RI "static void \fBfree_buckets\fP (struct flex_array *buckets)"
.br
.ti -1c
.RI "static void \fB__table_instance_destroy\fP (struct \fBtable_instance\fP *ti)"
.br
.ti -1c
.RI "static struct \fBtable_instance\fP * \fBtable_instance_alloc\fP (int new_size)"
.br
.ti -1c
.RI "static void \fBmask_array_rcu_cb\fP (struct rcu_head *rcu)"
.br
.ti -1c
.RI "static struct \fBmask_array\fP * \fBtbl_mask_array_alloc\fP (int size)"
.br
.ti -1c
.RI "static int \fBtbl_mask_array_realloc\fP (struct \fBflow_table\fP *tbl, int size)"
.br
.ti -1c
.RI "int \fBovs_flow_tbl_init\fP (struct \fBflow_table\fP *table)"
.br
.ti -1c
.RI "static void \fBflow_tbl_destroy_rcu_cb\fP (struct rcu_head *rcu)"
.br
.ti -1c
.RI "static void \fBtable_instance_destroy\fP (struct \fBtable_instance\fP *ti, struct \fBtable_instance\fP *ufid_ti, \fBbool\fP deferred)"
.br
.ti -1c
.RI "void \fBovs_flow_tbl_destroy\fP (struct \fBflow_table\fP *table)"
.br
.ti -1c
.RI "struct \fBsw_flow\fP * \fBovs_flow_tbl_dump_next\fP (struct \fBtable_instance\fP *ti, u32 *bucket, u32 *last)"
.br
.ti -1c
.RI "static struct hlist_head * \fBfind_bucket\fP (struct \fBtable_instance\fP *ti, u32 hash)"
.br
.ti -1c
.RI "static void \fBtable_instance_insert\fP (struct \fBtable_instance\fP *ti, struct \fBsw_flow\fP *flow)"
.br
.ti -1c
.RI "static void \fBufid_table_instance_insert\fP (struct \fBtable_instance\fP *ti, struct \fBsw_flow\fP *flow)"
.br
.ti -1c
.RI "static void \fBflow_table_copy_flows\fP (struct \fBtable_instance\fP *old, struct \fBtable_instance\fP *new, \fBbool\fP ufid)"
.br
.ti -1c
.RI "static struct \fBtable_instance\fP * \fBtable_instance_rehash\fP (struct \fBtable_instance\fP *ti, int n_buckets, \fBbool\fP ufid)"
.br
.ti -1c
.RI "int \fBovs_flow_tbl_flush\fP (struct \fBflow_table\fP *\fBflow_table\fP)"
.br
.ti -1c
.RI "static u32 \fBflow_hash\fP (const struct \fBsw_flow_key\fP *key, const struct \fBsw_flow_key_range\fP *range)"
.br
.ti -1c
.RI "static int \fBflow_key_start\fP (const struct \fBsw_flow_key\fP *key)"
.br
.ti -1c
.RI "static \fBbool\fP \fBcmp_key\fP (const struct \fBsw_flow_key\fP *key1, const struct \fBsw_flow_key\fP *key2, int key_start, int key_end)"
.br
.ti -1c
.RI "static \fBbool\fP \fBflow_cmp_masked_key\fP (const struct \fBsw_flow\fP *flow, const struct \fBsw_flow_key\fP *key, const struct \fBsw_flow_key_range\fP *range)"
.br
.ti -1c
.RI "static \fBbool\fP \fBovs_flow_cmp_unmasked_key\fP (const struct \fBsw_flow\fP *flow, const struct \fBsw_flow_match\fP *match)"
.br
.ti -1c
.RI "static struct \fBsw_flow\fP * \fBmasked_flow_lookup\fP (struct \fBtable_instance\fP *ti, const struct \fBsw_flow_key\fP *unmasked, const struct \fBsw_flow_mask\fP *mask, u32 *n_mask_hit)"
.br
.ti -1c
.RI "static struct \fBsw_flow\fP * \fBflow_lookup\fP (struct \fBflow_table\fP *tbl, struct \fBtable_instance\fP *ti, const struct \fBmask_array\fP *ma, const struct \fBsw_flow_key\fP *key, u32 *n_mask_hit, u32 *index)"
.br
.ti -1c
.RI "struct \fBsw_flow\fP * \fBovs_flow_tbl_lookup_stats\fP (struct \fBflow_table\fP *tbl, const struct \fBsw_flow_key\fP *key, u32 skb_hash, u32 *n_mask_hit)"
.br
.ti -1c
.RI "struct \fBsw_flow\fP * \fBovs_flow_tbl_lookup\fP (struct \fBflow_table\fP *tbl, const struct \fBsw_flow_key\fP *key)"
.br
.ti -1c
.RI "struct \fBsw_flow\fP * \fBovs_flow_tbl_lookup_exact\fP (struct \fBflow_table\fP *tbl, const struct \fBsw_flow_match\fP *match)"
.br
.ti -1c
.RI "static u32 \fBufid_hash\fP (const struct \fBsw_flow_id\fP *sfid)"
.br
.ti -1c
.RI "static \fBbool\fP \fBovs_flow_cmp_ufid\fP (const struct \fBsw_flow\fP *flow, const struct \fBsw_flow_id\fP *sfid)"
.br
.ti -1c
.RI "\fBbool\fP \fBovs_flow_cmp\fP (const struct \fBsw_flow\fP *flow, const struct \fBsw_flow_match\fP *match)"
.br
.ti -1c
.RI "struct \fBsw_flow\fP * \fBovs_flow_tbl_lookup_ufid\fP (struct \fBflow_table\fP *tbl, const struct \fBsw_flow_id\fP *ufid)"
.br
.ti -1c
.RI "int \fBovs_flow_tbl_num_masks\fP (const struct \fBflow_table\fP *table)"
.br
.ti -1c
.RI "static struct \fBtable_instance\fP * \fBtable_instance_expand\fP (struct \fBtable_instance\fP *ti, \fBbool\fP ufid)"
.br
.ti -1c
.RI "static void \fBtbl_mask_array_delete_mask\fP (struct \fBmask_array\fP *ma, struct \fBsw_flow_mask\fP *mask)"
.br
.ti -1c
.RI "static void \fBflow_mask_remove\fP (struct \fBflow_table\fP *tbl, struct \fBsw_flow_mask\fP *mask)"
.br
.ti -1c
.RI "void \fBovs_flow_tbl_remove\fP (struct \fBflow_table\fP *table, struct \fBsw_flow\fP *flow)"
.br
.ti -1c
.RI "static struct \fBsw_flow_mask\fP * \fBmask_alloc\fP (void)"
.br
.ti -1c
.RI "static \fBbool\fP \fBmask_equal\fP (const struct \fBsw_flow_mask\fP *a, const struct \fBsw_flow_mask\fP *b)"
.br
.ti -1c
.RI "static struct \fBsw_flow_mask\fP * \fBflow_mask_find\fP (const struct \fBflow_table\fP *tbl, const struct \fBsw_flow_mask\fP *mask)"
.br
.ti -1c
.RI "static int \fBflow_mask_insert\fP (struct \fBflow_table\fP *tbl, struct \fBsw_flow\fP *flow, const struct \fBsw_flow_mask\fP *new)"
.br
.ti -1c
.RI "static void \fBflow_key_insert\fP (struct \fBflow_table\fP *table, struct \fBsw_flow\fP *flow)"
.br
.ti -1c
.RI "static void \fBflow_ufid_insert\fP (struct \fBflow_table\fP *table, struct \fBsw_flow\fP *flow)"
.br
.ti -1c
.RI "int \fBovs_flow_tbl_insert\fP (struct \fBflow_table\fP *table, struct \fBsw_flow\fP *flow, const struct \fBsw_flow_mask\fP *mask)"
.br
.ti -1c
.RI "int \fBovs_flow_init\fP (void)"
.br
.ti -1c
.RI "void \fBovs_flow_exit\fP (void)"
.br
.in -1c
.SS "Variables"

.in +1c
.ti -1c
.RI "static struct kmem_cache * \fBflow_cache\fP"
.br
.ti -1c
.RI "struct kmem_cache *\fBflow_stats_cache\fP \fB__read_mostly\fP"
.br
.in -1c
.SH "Macro Definition Documentation"
.PP 
.SS "#define MASK_ARRAY_SIZE_MIN   16"

.SS "#define MC_HASH_ENTRIES   (1u << MC_HASH_SHIFT)"

.SS "#define MC_HASH_SEGS   ((sizeof(uint32_t) * 8) / \fBMC_HASH_SHIFT\fP)"

.SS "#define MC_HASH_SHIFT   8"

.SS "#define REHASH_INTERVAL   (10 * 60 * HZ)"

.SS "#define TBL_MIN_BUCKETS   1024"

.SH "Function Documentation"
.PP 
.SS "static void __table_instance_destroy (struct \fBtable_instance\fP * ti)\fC [static]\fP"

.PP
.nf
193 {
194     free_buckets(ti->buckets);
195     kfree(ti);
196 }
.fi
.SS "static struct flex_array* alloc_buckets (unsigned int n_buckets)\fC [static]\fP"

.PP
.nf
125 {
126     struct flex_array *buckets;
127     int i, err;
128 
129     buckets = flex_array_alloc(sizeof(struct hlist_head),
130                    n_buckets, GFP_KERNEL);
131     if (!buckets)
132         return NULL;
133 
134     err = flex_array_prealloc(buckets, 0, n_buckets, GFP_KERNEL);
135     if (err) {
136         flex_array_free(buckets);
137         return NULL;
138     }
139 
140     for (i = 0; i < n_buckets; i++)
141         INIT_HLIST_HEAD((struct hlist_head *)
142                     flex_array_get(buckets, i));
143 
144     return buckets;
145 }
.fi
.SS "static \fBbool\fP cmp_key (const struct \fBsw_flow_key\fP * key1, const struct \fBsw_flow_key\fP * key2, int key_start, int key_end)\fC [static]\fP"

.PP
.nf
517 {
518     const long *cp1 = (const long *)((const u8 *)key1 + key_start);
519     const long *cp2 = (const long *)((const u8 *)key2 + key_start);
520     long diffs = 0;
521     int i;
522 
523     for (i = key_start; i < key_end;  i += sizeof(long))
524         diffs |= *cp1++ ^ *cp2++;
525 
526     return diffs == 0;
527 }
.fi
.SS "static struct hlist_head* find_bucket (struct \fBtable_instance\fP * ti, u32 hash)\fC [static]\fP"

.PP
.nf
395 {
396     hash = jhash_1word(hash, ti->hash_seed);
397     return flex_array_get(ti->buckets,
398                 (hash & (ti->n_buckets - 1)));
399 }
.fi
.SS "static \fBbool\fP flow_cmp_masked_key (const struct \fBsw_flow\fP * flow, const struct \fBsw_flow_key\fP * key, const struct \fBsw_flow_key_range\fP * range)\fC [static]\fP"

.PP
.nf
532 {
533     return cmp_key(&flow->key, key, range->start, range->end);
534 }
.fi
.SS "static void flow_free (struct \fBsw_flow\fP * flow)\fC [static]\fP"

.PP
.nf
148 {
149     int node;
150 
151     if (ovs_identifier_is_key(&flow->id))
152         kfree(flow->id\&.unmasked_key);
153     kfree(rcu_dereference_raw(flow->sf_acts));
154     for_each_node(node)
155         if (flow->stats[node])
156             kmem_cache_free(flow_stats_cache,
157                     rcu_dereference_raw(flow->stats[node]));
158     kmem_cache_free(flow_cache, flow);
159 }
.fi
.SS "static u32 flow_hash (const struct \fBsw_flow_key\fP * key, const struct \fBsw_flow_key_range\fP * range)\fC [static]\fP"

.PP
.nf
493 {
494     int key_start = range->start;
495     int key_end = range->end;
496     const u32 *hash_key = (const u32 *)((const u8 *)key + key_start);
497     int hash_u32s = (key_end - key_start) >> 2;
498 
499     /* Make sure number of hash bytes are multiple of u32\&. */
500     BUILD_BUG_ON(sizeof(long) % sizeof(u32));
501 
502     return jhash2(hash_key, hash_u32s, 0);
503 }
.fi
.SS "static void flow_key_insert (struct \fBflow_table\fP * table, struct \fBsw_flow\fP * flow)\fC [static]\fP"

.PP
.nf
922 {
923     struct table_instance *new_ti = NULL;
924     struct table_instance *ti;
925 
926     flow->flow_table\&.hash = flow_hash(&flow->key, &flow->mask->range);
927     ti = ovsl_dereference(table->ti);
928     table_instance_insert(ti, flow);
929     table->count++;
930 
931     /* Expand table, if necessary, to make room\&. */
932     if (table->count > ti->n_buckets)
933         new_ti = table_instance_expand(ti, false);
934     else if (time_after(jiffies, table->last_rehash + REHASH_INTERVAL))
935         new_ti = table_instance_rehash(ti, ti->n_buckets, false);
936 
937     if (new_ti) {
938         rcu_assign_pointer(table->ti, new_ti);
939         call_rcu(&ti->rcu, flow_tbl_destroy_rcu_cb);
940         table->last_rehash = jiffies;
941     }
942 }
.fi
.SS "static int flow_key_start (const struct \fBsw_flow_key\fP * key)\fC [static]\fP"

.PP
.nf
506 {
507     if (key->tun_key\&.ipv4_dst)
508         return 0;
509     else
510         return rounddown(offsetof(struct sw_flow_key, phy),
511                       sizeof(long));
512 }
.fi
.SS "static struct \fBsw_flow\fP* flow_lookup (struct \fBflow_table\fP * tbl, struct \fBtable_instance\fP * ti, const struct \fBmask_array\fP * ma, const struct \fBsw_flow_key\fP * key, u32 * n_mask_hit, u32 * index)\fC [static]\fP"

.PP
.nf
578 {
579     struct sw_flow_mask *mask;
580     struct sw_flow *flow;
581     int i;
582 
583     if (*index < ma->max) {
584         mask = rcu_dereference_ovsl(ma->masks[*index]);
585         if (mask) {
586             flow = masked_flow_lookup(ti, key, mask, n_mask_hit);
587             if (flow)
588                 return flow;
589         }
590     }
591 
592     for (i = 0; i < ma->max; i++)  {
593 
594         if (i == *index)
595             continue;
596 
597         mask = rcu_dereference_ovsl(ma->masks[i]);
598         if (!mask)
599             continue;
600 
601         flow = masked_flow_lookup(ti, key, mask, n_mask_hit);
602         if (flow) { /* Found */
603             *index = i;
604             return flow;
605         }
606     }
607 
608     return NULL;
609 }
.fi
.SS "static struct \fBsw_flow_mask\fP* flow_mask_find (const struct \fBflow_table\fP * tbl, const struct \fBsw_flow_mask\fP * mask)\fC [static]\fP"

.PP
.nf
851 {
852     struct mask_array *ma;
853     int i;
854 
855     ma = ovsl_dereference(tbl->mask_array);
856     for (i = 0; i < ma->max; i++) {
857         struct sw_flow_mask *t;
858 
859         t = ovsl_dereference(ma->masks[i]);
860         if (t && mask_equal(mask, t))
861             return t;
862     }
863 
864     return NULL;
865 }
.fi
.SS "static int flow_mask_insert (struct \fBflow_table\fP * tbl, struct \fBsw_flow\fP * flow, const struct \fBsw_flow_mask\fP * new)\fC [static]\fP"

.PP
.nf
870 {
871     struct sw_flow_mask *mask;
872 
873     mask = flow_mask_find(tbl, new);
874     if (!mask) {
875         struct mask_array *ma;
876         int i;
877 
878         /* Allocate a new mask if none exsits\&. */
879         mask = mask_alloc();
880         if (!mask)
881             return -ENOMEM;
882 
883         mask->key = new->key;
884         mask->range = new->range;
885 
886         /* Add mask to mask-list\&. */
887         ma = ovsl_dereference(tbl->mask_array);
888         if (ma->count >= ma->max) {
889             int err;
890 
891             err = tbl_mask_array_realloc(tbl, ma->max +
892                               MASK_ARRAY_SIZE_MIN);
893             if (err) {
894                 kfree(mask);
895                 return err;
896             }
897             ma = ovsl_dereference(tbl->mask_array);
898         }
899 
900         for (i = 0; i < ma->max; i++) {
901             struct sw_flow_mask *t;
902 
903             t = ovsl_dereference(ma->masks[i]);
904             if (!t) {
905                 rcu_assign_pointer(ma->masks[i], mask);
906                 ma->count++;
907                 break;
908             }
909         }
910 
911     } else {
912         BUG_ON(!mask->ref_count);
913         mask->ref_count++;
914     }
915 
916     flow->mask = mask;
917     return 0;
918 }
.fi
.SS "static void flow_mask_remove (struct \fBflow_table\fP * tbl, struct \fBsw_flow_mask\fP * mask)\fC [static]\fP"

.PP
.nf
783 {
784     if (mask) {
785         /* ovs-lock is required to protect mask-refcount and
786          * mask list\&.
787          */
788         ASSERT_OVSL();
789         BUG_ON(!mask->ref_count);
790         mask->ref_count--;
791 
792         if (!mask->ref_count) {
793             struct mask_array *ma;
794 
795             ma = ovsl_dereference(tbl->mask_array);
796             tbl_mask_array_delete_mask(ma, mask);
797 
798             /* Shrink the mask array if necessary\&. */
799             if (ma->max >= (MASK_ARRAY_SIZE_MIN * 2) &&
800                 ma->count <= (ma->max / 3))
801                 tbl_mask_array_realloc(tbl, ma->max / 2);
802 
803         }
804     }
805 }
.fi
.SS "static void flow_table_copy_flows (struct \fBtable_instance\fP * old, struct \fBtable_instance\fP * new, \fBbool\fP ufid)\fC [static]\fP"

.PP
.nf
421 {
422     int old_ver;
423     int i;
424 
425     old_ver = old->node_ver;
426     new->node_ver = !old_ver;
427 
428     /* Insert in new table\&. */
429     for (i = 0; i < old->n_buckets; i++) {
430         struct sw_flow *flow;
431         struct hlist_head *head;
432 
433         head = flex_array_get(old->buckets, i);
434 
435         if (ufid)
436             hlist_for_each_entry(flow, head,
437                          ufid_table\&.node[old_ver])
438                 ufid_table_instance_insert(new, flow);
439         else
440             hlist_for_each_entry(flow, head,
441                          flow_table\&.node[old_ver])
442                 table_instance_insert(new, flow);
443     }
444 
445     old->keep_flows = true;
446 }
.fi
.SS "static void flow_tbl_destroy_rcu_cb (struct rcu_head * rcu)\fC [static]\fP"

.PP
.nf
310 {
311     struct table_instance *ti = container_of(rcu, struct table_instance, rcu);
312 
313     __table_instance_destroy(ti);
314 }
.fi
.SS "static void flow_ufid_insert (struct \fBflow_table\fP * table, struct \fBsw_flow\fP * flow)\fC [static]\fP"

.PP
.nf
946 {
947     struct table_instance *ti;
948 
949     flow->ufid_table\&.hash = ufid_hash(&flow->id);
950     ti = ovsl_dereference(table->ufid_ti);
951     ufid_table_instance_insert(ti, flow);
952     table->ufid_count++;
953 
954     /* Expand table, if necessary, to make room\&. */
955     if (table->ufid_count > ti->n_buckets) {
956         struct table_instance *new_ti;
957 
958         new_ti = table_instance_expand(ti, true);
959         if (new_ti) {
960             rcu_assign_pointer(table->ufid_ti, new_ti);
961             call_rcu(&ti->rcu, flow_tbl_destroy_rcu_cb);
962         }
963     }
964 }
.fi
.SS "static void free_buckets (struct flex_array * buckets)\fC [static]\fP"

.PP
.nf
187 {
188     flex_array_free(buckets);
189 }
.fi
.SS "static struct \fBsw_flow_mask\fP* mask_alloc (void)\fC [static]\fP"

.PP
.nf
828 {
829     struct sw_flow_mask *mask;
830 
831     mask = kmalloc(sizeof(*mask), GFP_KERNEL);
832     if (mask)
833         mask->ref_count = 1;
834 
835     return mask;
836 }
.fi
.SS "static void mask_array_rcu_cb (struct rcu_head * rcu)\fC [static]\fP"

.PP
.nf
220 {
221     struct mask_array *ma = container_of(rcu, struct mask_array, rcu);
222 
223     kfree(ma);
224 }
.fi
.SS "static \fBbool\fP mask_equal (const struct \fBsw_flow_mask\fP * a, const struct \fBsw_flow_mask\fP * b)\fC [static]\fP"

.PP
.nf
840 {
841     const u8 *a_ = (const u8 *)&a->key + a->range\&.start;
842     const u8 *b_ = (const u8 *)&b->key + b->range\&.start;
843 
844     return  (a->range\&.end == b->range\&.end)
845         && (a->range\&.start == b->range\&.start)
846         && (memcmp(a_, b_, range_n_bytes(&a->range)) == 0);
847 }
.fi
.SS "static struct \fBsw_flow\fP* masked_flow_lookup (struct \fBtable_instance\fP * ti, const struct \fBsw_flow_key\fP * unmasked, const struct \fBsw_flow_mask\fP * mask, u32 * n_mask_hit)\fC [static]\fP"

.PP
.nf
551 {
552     struct sw_flow *flow;
553     struct hlist_head *head;
554     u32 hash;
555     struct sw_flow_key masked_key;
556 
557     ovs_flow_mask_key(&masked_key, unmasked, mask);
558     hash = flow_hash(&masked_key, &mask->range);
559     head = find_bucket(ti, hash);
560     (*n_mask_hit)++;
561     hlist_for_each_entry_rcu(flow, head, flow_table\&.node[ti->node_ver]) {
562         if (flow->mask == mask && flow->flow_table\&.hash == hash &&
563             flow_cmp_masked_key(flow, &masked_key, &mask->range))
564             return flow;
565     }
566     return NULL;
567 }
.fi
.SS "struct \fBsw_flow\fP* ovs_flow_alloc (void)"

.PP
.nf
84 {
85     struct sw_flow *flow;
86     struct flow_stats *stats;
87     int node;
88 
89     flow = kmem_cache_alloc(flow_cache, GFP_KERNEL);
90     if (!flow)
91         return ERR_PTR(-ENOMEM);
92 
93     flow->sf_acts = NULL;
94     flow->mask = NULL;
95     flow->id\&.ufid_len = 0;
96     flow->id\&.unmasked_key = NULL;
97     flow->stats_last_writer = NUMA_NO_NODE;
98 
99     /* Initialize the default stat node\&. */
100     stats = kmem_cache_alloc_node(flow_stats_cache,
101                       GFP_KERNEL | __GFP_ZERO, 0);
102     if (!stats)
103         goto err;
104 
105     spin_lock_init(&stats->lock);
106 
107     RCU_INIT_POINTER(flow->stats[0], stats);
108 
109     for_each_node(node)
110         if (node != 0)
111             RCU_INIT_POINTER(flow->stats[node], NULL);
112 
113     return flow;
114 err:
115     kmem_cache_free(flow_cache, flow);
116     return ERR_PTR(-ENOMEM);
117 }
.fi
.SS "\fBbool\fP ovs_flow_cmp (const struct \fBsw_flow\fP * flow, const struct \fBsw_flow_match\fP * match)"

.PP
.nf
725 {
726     if (ovs_identifier_is_ufid(&flow->id))
727         return flow_cmp_masked_key(flow, match->key, &match->range);
728 
729     return ovs_flow_cmp_unmasked_key(flow, match);
730 }
.fi
.SS "static \fBbool\fP ovs_flow_cmp_ufid (const struct \fBsw_flow\fP * flow, const struct \fBsw_flow_id\fP * sfid)\fC [static]\fP"

.PP
.nf
717 {
718     if (flow->id\&.ufid_len != sfid->ufid_len)
719         return false;
720 
721     return !memcmp(flow->id\&.ufid, sfid->ufid, sfid->ufid_len);
722 }
.fi
.SS "static \fBbool\fP ovs_flow_cmp_unmasked_key (const struct \fBsw_flow\fP * flow, const struct \fBsw_flow_match\fP * match)\fC [static]\fP"

.PP
.nf
538 {
539     struct sw_flow_key *key = match->key;
540     int key_start = flow_key_start(key);
541     int key_end = match->range\&.end;
542 
543     BUG_ON(ovs_identifier_is_ufid(&flow->id));
544     return cmp_key(flow->id\&.unmasked_key, key, key_start, key_end);
545 }
.fi
.SS "void ovs_flow_exit (void)"

.PP
.nf
1011 {
1012     kmem_cache_destroy(flow_stats_cache);
1013     kmem_cache_destroy(flow_cache);
1014 }
.fi
.SS "void ovs_flow_free (struct \fBsw_flow\fP * flow, \fBbool\fP deferred)"

.PP
.nf
176 {
177     if (!flow)
178         return;
179 
180     if (deferred)
181         call_rcu(&flow->rcu, rcu_free_flow_callback);
182     else
183         flow_free(flow);
184 }
.fi
.SS "int ovs_flow_init (void)"

.PP
.nf
986 {
987     BUILD_BUG_ON(__alignof__(struct sw_flow_key) % __alignof__(long));
988     BUILD_BUG_ON(sizeof(struct sw_flow_key) % sizeof(long));
989 
990     flow_cache = kmem_cache_create("sw_flow", sizeof(struct sw_flow)
991                        + (nr_node_ids
992                       * sizeof(struct flow_stats *)),
993                        0, 0, NULL);
994     if (flow_cache == NULL)
995         return -ENOMEM;
996 
997     flow_stats_cache
998         = kmem_cache_create("sw_flow_stats", sizeof(struct flow_stats),
999                     0, SLAB_HWCACHE_ALIGN, NULL);
1000     if (flow_stats_cache == NULL) {
1001         kmem_cache_destroy(flow_cache);
1002         flow_cache = NULL;
1003         return -ENOMEM;
1004     }
1005 
1006     return 0;
1007 }
.fi
.SS "void ovs_flow_mask_key (struct \fBsw_flow_key\fP * dst, const struct \fBsw_flow_key\fP * src, const struct \fBsw_flow_mask\fP * mask)"

.PP
.nf
67 {
68     const long *m = (const long *)((const u8 *)&mask->key +
69                 mask->range\&.start);
70     const long *s = (const long *)((const u8 *)src +
71                 mask->range\&.start);
72     long *d = (long *)((u8 *)dst + mask->range\&.start);
73     int i;
74 
75     /* The memory outside of the 'mask->range' are not set since
76      * further operations on 'dst' only uses contents within
77      * 'mask->range'\&.
78      */
79     for (i = 0; i < range_n_bytes(&mask->range); i += sizeof(long))
80         *d++ = *s++ & *m++;
81 }
.fi
.SS "int ovs_flow_tbl_count (const struct \fBflow_table\fP * table)"

.PP
.nf
120 {
121     return table->count;
122 }
.fi
.SS "void ovs_flow_tbl_destroy (struct \fBflow_table\fP * table)"

.PP
.nf
358 {
359     struct table_instance *ti = rcu_dereference_raw(table->ti);
360     struct table_instance *ufid_ti = rcu_dereference_raw(table->ufid_ti);
361 
362     free_percpu(table->mask_cache);
363     kfree(rcu_dereference_raw(table->mask_array));
364     table_instance_destroy(ti, ufid_ti, false);
365 }
.fi
.SS "struct \fBsw_flow\fP* ovs_flow_tbl_dump_next (struct \fBtable_instance\fP * ti, u32 * bucket, u32 * last)"

.PP
.nf
369 {
370     struct sw_flow *flow;
371     struct hlist_head *head;
372     int ver;
373     int i;
374 
375     ver = ti->node_ver;
376     while (*bucket < ti->n_buckets) {
377         i = 0;
378         head = flex_array_get(ti->buckets, *bucket);
379         hlist_for_each_entry_rcu(flow, head, flow_table\&.node[ver]) {
380             if (i < *last) {
381                 i++;
382                 continue;
383             }
384             *last = i + 1;
385             return flow;
386         }
387         (*bucket)++;
388         *last = 0;
389     }
390 
391     return NULL;
392 }
.fi
.SS "int ovs_flow_tbl_flush (struct \fBflow_table\fP * flow_table)"

.PP
.nf
463 {
464     struct table_instance *old_ti, *new_ti;
465     struct table_instance *old_ufid_ti, *new_ufid_ti;
466 
467     new_ti = table_instance_alloc(TBL_MIN_BUCKETS);
468     if (!new_ti)
469         return -ENOMEM;
470     new_ufid_ti = table_instance_alloc(TBL_MIN_BUCKETS);
471     if (!new_ufid_ti)
472         goto err_free_ti;
473 
474     old_ti = ovsl_dereference(flow_table->ti);
475     old_ufid_ti = ovsl_dereference(flow_table->ufid_ti);
476 
477     rcu_assign_pointer(flow_table->ti, new_ti);
478     rcu_assign_pointer(flow_table->ufid_ti, new_ufid_ti);
479     flow_table->last_rehash = jiffies;
480     flow_table->count = 0;
481     flow_table->ufid_count = 0;
482 
483     table_instance_destroy(old_ti, old_ufid_ti, true);
484     return 0;
485 
486 err_free_ti:
487     __table_instance_destroy(new_ti);
488     return -ENOMEM;
489 }
.fi
.SS "int ovs_flow_tbl_init (struct \fBflow_table\fP * table)"

.PP
.nf
271 {
272     struct table_instance *ti, *ufid_ti;
273     struct mask_array *ma;
274 
275     table->mask_cache = __alloc_percpu(sizeof(struct mask_cache_entry) *
276                       MC_HASH_ENTRIES, __alignof__(struct mask_cache_entry));
277     if (!table->mask_cache)
278         return -ENOMEM;
279 
280     ma = tbl_mask_array_alloc(MASK_ARRAY_SIZE_MIN);
281     if (!ma)
282         goto free_mask_cache;
283 
284     ti = table_instance_alloc(TBL_MIN_BUCKETS);
285     if (!ti)
286         goto free_mask_array;
287 
288     ufid_ti = table_instance_alloc(TBL_MIN_BUCKETS);
289     if (!ufid_ti)
290         goto free_ti;
291 
292     rcu_assign_pointer(table->ti, ti);
293     rcu_assign_pointer(table->ufid_ti, ufid_ti);
294     rcu_assign_pointer(table->mask_array, ma);
295     table->last_rehash = jiffies;
296     table->count = 0;
297     table->ufid_count = 0;
298     return 0;
299 
300 free_ti:
301     __table_instance_destroy(ti);
302 free_mask_array:
303     kfree(ma);
304 free_mask_cache:
305     free_percpu(table->mask_cache);
306     return -ENOMEM;
307 }
.fi
.SS "int ovs_flow_tbl_insert (struct \fBflow_table\fP * table, struct \fBsw_flow\fP * flow, const struct \fBsw_flow_mask\fP * mask)"

.PP
.nf
969 {
970     int err;
971 
972     err = flow_mask_insert(table, flow, mask);
973     if (err)
974         return err;
975     flow_key_insert(table, flow);
976     if (ovs_identifier_is_ufid(&flow->id))
977         flow_ufid_insert(table, flow);
978 
979     return 0;
980 }
.fi
.SS "struct \fBsw_flow\fP* ovs_flow_tbl_lookup (struct \fBflow_table\fP * tbl, const struct \fBsw_flow_key\fP * key)"

.PP
.nf
677 {
678     struct table_instance *ti = rcu_dereference_ovsl(tbl->ti);
679     struct mask_array *ma = rcu_dereference_ovsl(tbl->mask_array);
680     u32 __always_unused n_mask_hit;
681     u32 index = 0;
682 
683     return flow_lookup(tbl, ti, ma, key, &n_mask_hit, &index);
684 }
.fi
.SS "struct \fBsw_flow\fP* ovs_flow_tbl_lookup_exact (struct \fBflow_table\fP * tbl, const struct \fBsw_flow_match\fP * match)"

.PP
.nf
688 {
689     struct mask_array *ma = ovsl_dereference(tbl->mask_array);
690     int i;
691 
692     /* Always called under ovs-mutex\&. */
693     for (i = 0; i < ma->max; i++) {
694         struct table_instance *ti = ovsl_dereference(tbl->ti);
695         u32 __always_unused n_mask_hit;
696         struct sw_flow_mask *mask;
697         struct sw_flow *flow;
698 
699         mask = ovsl_dereference(ma->masks[i]);
700         if (!mask)
701             continue;
702         flow = masked_flow_lookup(ti, match->key, mask, &n_mask_hit);
703         if (flow && ovs_identifier_is_key(&flow->id) &&
704             ovs_flow_cmp_unmasked_key(flow, match))
705             return flow;
706     }
707     return NULL;
708 }
.fi
.SS "struct \fBsw_flow\fP* ovs_flow_tbl_lookup_stats (struct \fBflow_table\fP * tbl, const struct \fBsw_flow_key\fP * key, u32 skb_hash, u32 * n_mask_hit)"

.PP
.nf
622 {
623     struct mask_array *ma = rcu_dereference(tbl->mask_array);
624     struct table_instance *ti = rcu_dereference(tbl->ti);
625     struct mask_cache_entry *entries, *ce;
626     struct sw_flow *flow;
627     u32 hash;
628     int seg;
629 
630     *n_mask_hit = 0;
631     if (unlikely(!skb_hash)) {
632         u32 mask_index = 0;
633 
634         return flow_lookup(tbl, ti, ma, key, n_mask_hit, &mask_index);
635     }
636 
637     /* Pre and post recirulation flows usually have the same skb_hash
638      * value\&. To avoid hash collisions, rehash the 'skb_hash' with
639      * 'recirc_id'\&.  */
640     if (key->recirc_id)
641         skb_hash = jhash_1word(skb_hash, key->recirc_id);
642 
643     ce = NULL;
644     hash = skb_hash;
645     entries = this_cpu_ptr(tbl->mask_cache);
646 
647     /* Find the cache entry 'ce' to operate on\&. */
648     for (seg = 0; seg < MC_HASH_SEGS; seg++) {
649         int index = hash & (MC_HASH_ENTRIES - 1);
650         struct mask_cache_entry *e;
651 
652         e = &entries[index];
653         if (e->skb_hash == skb_hash) {
654             flow = flow_lookup(tbl, ti, ma, key, n_mask_hit,
655                        &e->mask_index);
656             if (!flow)
657                 e->skb_hash = 0;
658             return flow;
659         }
660 
661         if (!ce || e->skb_hash < ce->skb_hash)
662             ce = e;  /* A better replacement cache candidate\&. */
663 
664         hash >>= MC_HASH_SHIFT;
665     }
666 
667     /* Cache miss, do full lookup\&. */
668     flow = flow_lookup(tbl, ti, ma, key, n_mask_hit, &ce->mask_index);
669     if (flow)
670         ce->skb_hash = skb_hash;
671 
672     return flow;
673 }
.fi
.SS "struct \fBsw_flow\fP* ovs_flow_tbl_lookup_ufid (struct \fBflow_table\fP * tbl, const struct \fBsw_flow_id\fP * ufid)"

.PP
.nf
734 {
735     struct table_instance *ti = rcu_dereference_ovsl(tbl->ufid_ti);
736     struct sw_flow *flow;
737     struct hlist_head *head;
738     u32 hash;
739 
740     hash = ufid_hash(ufid);
741     head = find_bucket(ti, hash);
742     hlist_for_each_entry_rcu(flow, head, ufid_table\&.node[ti->node_ver]) {
743         if (flow->ufid_table\&.hash == hash &&
744             ovs_flow_cmp_ufid(flow, ufid))
745             return flow;
746     }
747     return NULL;
748 }
.fi
.SS "int ovs_flow_tbl_num_masks (const struct \fBflow_table\fP * table)"

.PP
.nf
751 {
752     struct mask_array *ma;
753 
754     ma = rcu_dereference_ovsl(table->mask_array);
755     return ma->count;
756 }
.fi
.SS "void ovs_flow_tbl_remove (struct \fBflow_table\fP * table, struct \fBsw_flow\fP * flow)"

.PP
.nf
809 {
810     struct table_instance *ti = ovsl_dereference(table->ti);
811     struct table_instance *ufid_ti = ovsl_dereference(table->ufid_ti);
812 
813     BUG_ON(table->count == 0);
814     hlist_del_rcu(&flow->flow_table\&.node[ti->node_ver]);
815     table->count--;
816     if (ovs_identifier_is_ufid(&flow->id)) {
817         hlist_del_rcu(&flow->ufid_table\&.node[ufid_ti->node_ver]);
818         table->ufid_count--;
819     }
820 
821     /* RCU delete the mask\&. 'flow->mask' is not NULLed, as it should be
822      * accessible as long as the RCU read lock is held\&.
823      */
824     flow_mask_remove(table, flow->mask);
825 }
.fi
.SS "static u16 range_n_bytes (const struct \fBsw_flow_key_range\fP * range)\fC [static]\fP"

.PP
.nf
61 {
62     return range->end - range->start;
63 }
.fi
.SS "static void rcu_free_flow_callback (struct rcu_head * rcu)\fC [static]\fP"

.PP
.nf
162 {
163     struct sw_flow *flow = container_of(rcu, struct sw_flow, rcu);
164 
165     flow_free(flow);
166 }
.fi
.SS "static void rcu_free_sw_flow_mask_cb (struct rcu_head * rcu)\fC [static]\fP"

.PP
.nf
169 {
170     struct sw_flow_mask *mask = container_of(rcu, struct sw_flow_mask, rcu);
171 
172     kfree(mask);
173 }
.fi
.SS "static struct \fBtable_instance\fP* table_instance_alloc (int new_size)\fC [static]\fP"

.PP
.nf
199 {
200     struct table_instance *ti = kmalloc(sizeof(*ti), GFP_KERNEL);
201 
202     if (!ti)
203         return NULL;
204 
205     ti->buckets = alloc_buckets(new_size);
206 
207     if (!ti->buckets) {
208         kfree(ti);
209         return NULL;
210     }
211     ti->n_buckets = new_size;
212     ti->node_ver = 0;
213     ti->keep_flows = false;
214     get_random_bytes(&ti->hash_seed, sizeof(u32));
215 
216     return ti;
217 }
.fi
.SS "static void table_instance_destroy (struct \fBtable_instance\fP * ti, struct \fBtable_instance\fP * ufid_ti, \fBbool\fP deferred)\fC [static]\fP"

.PP
.nf
319 {
320     int i;
321 
322     if (!ti)
323         return;
324 
325     BUG_ON(!ufid_ti);
326     if (ti->keep_flows)
327         goto skip_flows;
328 
329     for (i = 0; i < ti->n_buckets; i++) {
330         struct sw_flow *flow;
331         struct hlist_head *head = flex_array_get(ti->buckets, i);
332         struct hlist_node *n;
333         int ver = ti->node_ver;
334         int ufid_ver = ufid_ti->node_ver;
335 
336         hlist_for_each_entry_safe(flow, n, head, flow_table\&.node[ver]) {
337             hlist_del_rcu(&flow->flow_table\&.node[ver]);
338             if (ovs_identifier_is_ufid(&flow->id))
339                 hlist_del_rcu(&flow->ufid_table\&.node[ufid_ver]);
340             ovs_flow_free(flow, deferred);
341         }
342     }
343 
344 skip_flows:
345     if (deferred) {
346         call_rcu(&ti->rcu, flow_tbl_destroy_rcu_cb);
347         call_rcu(&ufid_ti->rcu, flow_tbl_destroy_rcu_cb);
348     } else {
349         __table_instance_destroy(ti);
350         __table_instance_destroy(ufid_ti);
351     }
352 }
.fi
.SS "static struct \fBtable_instance\fP* table_instance_expand (struct \fBtable_instance\fP * ti, \fBbool\fP ufid)\fC [static]\fP"

.PP
.nf
760 {
761     return table_instance_rehash(ti, ti->n_buckets * 2, ufid);
762 }
.fi
.SS "static void table_instance_insert (struct \fBtable_instance\fP * ti, struct \fBsw_flow\fP * flow)\fC [static]\fP"

.PP
.nf
403 {
404     struct hlist_head *head;
405 
406     head = find_bucket(ti, flow->flow_table\&.hash);
407     hlist_add_head_rcu(&flow->flow_table\&.node[ti->node_ver], head);
408 }
.fi
.SS "static struct \fBtable_instance\fP* table_instance_rehash (struct \fBtable_instance\fP * ti, int n_buckets, \fBbool\fP ufid)\fC [static]\fP"

.PP
.nf
450 {
451     struct table_instance *new_ti;
452 
453     new_ti = table_instance_alloc(n_buckets);
454     if (!new_ti)
455         return NULL;
456 
457     flow_table_copy_flows(ti, new_ti, ufid);
458 
459     return new_ti;
460 }
.fi
.SS "static struct \fBmask_array\fP* tbl_mask_array_alloc (int size)\fC [static]\fP"

.PP
.nf
227 {
228     struct mask_array *new;
229 
230     size = max(MASK_ARRAY_SIZE_MIN, size);
231     new = kzalloc(sizeof(struct mask_array) +
232               sizeof(struct sw_flow_mask *) * size, GFP_KERNEL);
233     if (!new)
234         return NULL;
235 
236     new->count = 0;
237     new->max = size;
238 
239     return new;
240 }
.fi
.SS "static void tbl_mask_array_delete_mask (struct \fBmask_array\fP * ma, struct \fBsw_flow_mask\fP * mask)\fC [static]\fP"

.PP
.nf
766 {
767     int i;
768 
769     /* Remove the deleted mask pointers from the array */
770     for (i = 0; i < ma->max; i++) {
771         if (mask == ovsl_dereference(ma->masks[i])) {
772             RCU_INIT_POINTER(ma->masks[i], NULL);
773             ma->count--;
774             call_rcu(&mask->rcu, rcu_free_sw_flow_mask_cb);
775             return;
776         }
777     }
778     BUG();
779 }
.fi
.SS "static int tbl_mask_array_realloc (struct \fBflow_table\fP * tbl, int size)\fC [static]\fP"

.PP
.nf
243 {
244     struct mask_array *old;
245     struct mask_array *new;
246 
247     new = tbl_mask_array_alloc(size);
248     if (!new)
249         return -ENOMEM;
250 
251     old = ovsl_dereference(tbl->mask_array);
252     if (old) {
253         int i, count = 0;
254 
255         for (i = 0; i < old->max; i++) {
256             if (ovsl_dereference(old->masks[i]))
257                 new->masks[count++] = old->masks[i];
258         }
259 
260         new->count = count;
261     }
262     rcu_assign_pointer(tbl->mask_array, new);
263 
264     if (old)
265         call_rcu(&old->rcu, mask_array_rcu_cb);
266 
267     return 0;
268 }
.fi
.SS "static u32 ufid_hash (const struct \fBsw_flow_id\fP * sfid)\fC [static]\fP"

.PP
.nf
711 {
712     return jhash(sfid->ufid, sfid->ufid_len, 0);
713 }
.fi
.SS "static void ufid_table_instance_insert (struct \fBtable_instance\fP * ti, struct \fBsw_flow\fP * flow)\fC [static]\fP"

.PP
.nf
412 {
413     struct hlist_head *head;
414 
415     head = find_bucket(ti, flow->ufid_table\&.hash);
416     hlist_add_head_rcu(&flow->ufid_table\&.node[ti->node_ver], head);
417 }
.fi
.SH "Variable Documentation"
.PP 
.SS "struct kmem_cache* \fBflow_stats_cache\fP __read_mostly"

.SS "struct kmem_cache* flow_cache\fC [static]\fP"

.SH "Author"
.PP 
Generated automatically by Doxygen for ovs datapath from the source code\&.
